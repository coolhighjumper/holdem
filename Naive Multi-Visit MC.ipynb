{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief\n",
    "\n",
    "This notebook shows simple implementation of On-Policy Multi-Visit Monte Carlo decision maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Table\n",
    "`ActionTable` is a self-defined action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionTable():\n",
    "    CHECK = 1\n",
    "    CALL = 2\n",
    "    RAISE = 3\n",
    "    FOLD = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Model\n",
    "`MCModel` uses Monte Carlo to update its action policy when an episode terminates. An episode is a series of states and actions pairs. A state describes the observed environment after an action has been made. For example, we can use a combination of player's cards and public cards to represent a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCModel(object):\n",
    "    def __init__(self):\n",
    "        self.episode = []      # a series of (state, action) pairs\n",
    "        self.pi = {}           # action policy\n",
    "        self.Q = {}            # expectation of (state, action) pair\n",
    "        self.Returns = {}      # returns of each (state, action) pairs\n",
    "        self.initial_stack = 0  \n",
    "        self.final_stack = 0\n",
    "        self.epsilon = 0.3     # probability not to perform the best action (to do exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we describe each member function under `MCModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_state` function convert observation into a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_state(self, observation):\n",
    "        my_card = observation.player_states.hand    # a list of 2\n",
    "        community_card = observation.community_card # a list of 5\n",
    "        return my_card + community_card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`record_episode` is called from outside each time we want to observe. For example, we might want to call `record_episode` when we receive `__action` or `__bet` message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def record_episode(self, observation, action):\n",
    "        state = self.get_state(observation)\n",
    "        self.episode.append([','.join(map(str, state)), action.action])  # use string to represent state\n",
    "                                                                         # action is an instance of ActionTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`set_initial_stack` and `set_final_stack` are called from outside when a round starts and ends, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def set_initial_stack(self, stack):\n",
    "        self.initial_stack = stack\n",
    "        \n",
    "    def set_final_stack(self, stack):\n",
    "        self.final_stack = stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`on_policy_mc` is called each time an episode ends, specifically, when a round ends. We here use stack difference as return of  the rounds, ie, how well the bot played.\n",
    "\n",
    "There are three `for` loops inside the `on_policy_mc` function. The first loop and second loop calculate the expected return each state, action pair by sampling. The third loop update the model's action policy `pi`. Note that the action policy not always choose the best action for a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def on_policy_mc(self):\n",
    "        G = self.final_stack - self.initial_stack  # stack difference as return\n",
    "        for sa in self.episode:\n",
    "            s, a = sa\n",
    "            # no discount, so G for all s,a are same, regardless of order\n",
    "            if s not in self.Returns:\n",
    "                self.Returns[s] = {}\n",
    "            if a not in self.Returns[s]:\n",
    "                self.Returns[s][a] = []\n",
    "            self.Returns[s][a].append(G)\n",
    "        \n",
    "        # update Q\n",
    "        for s in self.Returns.keys():\n",
    "            for a in self.Returns[s].keys():\n",
    "                if s not in self.Q:\n",
    "                    self.Q[s] = {}\n",
    "                self.Q[s][a] = np.mean(self.Returns[s][a])\n",
    "        \n",
    "        # update action policy pi\n",
    "        for s in self.Returns.keys():\n",
    "            possible_action = self.Returns[s]\n",
    "            A_star = max(possible_action.iteritems(), key=operator.itemgetter(1))[0]  # best action by largest average return\n",
    "            exploit_prob = 1 - self.epsilon + self.epsilon / 4  # four action only\n",
    "            explore_prob = self.epsilon / 4\n",
    "            choice = np.random.choice(['exploit', 'explore'], [exploit_prob, explore_prob])\n",
    "            if choice == 'exploit':\n",
    "                self.pi[s] = A_star\n",
    "            else:\n",
    "                self.pi[s] = random.choice([ActionTable.FOLD,\n",
    "                                            ActionTable.CALL,\n",
    "                                            ActionTable.RAISE,\n",
    "                                            ActionTable.CHECK])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `take_action` is called each time an action is needed. `take_action` will return an action suggested by action policy `pi`, or a randomly choose action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def take_action(self, observation):\n",
    "        # take action under pi\n",
    "        state = self.get_state(observation)\n",
    "        state_string = ','.join(map(str, state))\n",
    "        if state_string not in self.pi:\n",
    "            action = random.choice([ActionTable.FOLD,\n",
    "                                    ActionTable.CALL,\n",
    "                                    ActionTable.RAISE,\n",
    "                                    ActionTable.CHECK])\n",
    "            self.pi[state_string] = action\n",
    "        else:\n",
    "            action = self.pi[state_string]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
